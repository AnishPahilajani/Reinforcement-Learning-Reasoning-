{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oW2D1_PpNqF"
   },
   "source": [
    "# LoRA **GRPO** fine-tuning `Qwen2.5 0.5B` on a single T4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "znbQSsMqi7HJ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install uv\n",
    "!pip install trl\n",
    "!uv pip install --system triton==2.2.0\n",
    "!uv pip install --system vllm\n",
    "!uv pip install --system bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJXnqY58qhme",
    "outputId": "450673c0-54bc-4fc4-99b7-3347847b47c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 07:22:52 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-26 07:22:53 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bnc1lUleqmrk",
    "outputId": "53b46686-4da4-4f45-dcbf-5e67dba29d6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 7473 samples\n"
     ]
    }
   ],
   "source": [
    "R1_STYLE_SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user\n",
    "with the answer. The reasoning process and answer are enclosed within <reasoning> </reasoning> and\n",
    "<answer> </answer> tags, respectively, i.e., <reasoning> reasoning process here </reasoning>\n",
    "<answer> answer here </answer>.\"\"\"\n",
    "\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"The answer must be a single integer.\"\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset_name, split=\"train\", chunk_size=1000) -> Dataset:\n",
    "    dataset = load_dataset(dataset_name, 'main')[split]\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Loaded {total_samples} samples\")\n",
    "\n",
    "    def extract_hash_answer(text: str) -> str | None:\n",
    "        try:\n",
    "            return text.split(\"####\")[1].strip()\n",
    "        except IndexError:\n",
    "            return None\n",
    "\n",
    "    def process_batch(batch):\n",
    "        prompts = [[\n",
    "            {'role': 'system', 'content': R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS},\n",
    "            {'role': 'user', 'content': \"What is 2+2?\"},\n",
    "            {'role': 'assistant', 'content': \"<reasoning>To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.</reasoning>\\n<answer>4</answer>\"},\n",
    "            {'role': 'user', 'content': q.strip()}\n",
    "        ] for q in batch['question']]\n",
    "\n",
    "        return {\n",
    "            'prompt': prompts,\n",
    "            'answer': [extract_hash_answer(a) for a in batch['answer']]\n",
    "        }\n",
    "\n",
    "    return dataset.map(process_batch, batched=True, batch_size=chunk_size)\n",
    "\n",
    "dataset_name = 'openai/gsm8k'\n",
    "dataset = preprocess_dataset(dataset_name, chunk_size=500)\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    try:\n",
    "        answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
    "        return answer\n",
    "    except IndexError:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dpFmMr5ttC2"
   },
   "source": [
    "# Understanding the Dataset Structure\n",
    "\n",
    "This dataset represents a mathematical word problem along with its expected solution format. The main structure contains three key components:\n",
    "\n",
    "1. **Question Component**: Contains the primary word problem about Natalia's clip sales: \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\"\n",
    "\n",
    "2. **Answer Component**: Stores the numerical solution: 72\n",
    "\n",
    "3. **Prompt Component**: Demonstrates the expected solution format through a series of conversation examples:\n",
    "   - A system message outlining the required format using XML tags (`<reasoning>` and `<answer>`)\n",
    "   - A simple example using \"What is 2+2?\" to demonstrate proper formatting\n",
    "   - An assistant's response showing how to structure the reasoning and answer\n",
    "   - The original Natalia question repeated in the conversation format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PV8NhmXfrqRI",
    "outputId": "fbedb7dc-6449-4fc4-89e1-ad8790bbcaba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       " 'answer': '72',\n",
       " 'prompt': [{'content': 'A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\\nThe assistant first thinks about the reasoning process in the mind and then provides the user\\nwith the answer. The reasoning process and answer are enclosed within <reasoning> </reasoning> and\\n<answer> </answer> tags, respectively, i.e., <reasoning> reasoning process here </reasoning>\\n<answer> answer here </answer>.\\nThe answer must be a single integer.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'What is 2+2?', 'role': 'user'},\n",
       "  {'content': '<reasoning>To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.</reasoning>\\n<answer>4</answer>',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-G5sVin-t2Ap"
   },
   "source": [
    "# Reward functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mSqJAElwt0wt"
   },
   "outputs": [],
   "source": [
    "def format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has the correct format.\"\"\"\n",
    "    pattern = r\"^<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [bool(re.match(pattern, r)) for r in responses]\n",
    "    print(''.join('⭐' if match else '❌' for match in matches))\n",
    "    return [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "#Correctness function that shows only one generation\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the answer is correct.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print(f\"Question: {prompts[0][-1]['content']}\\nAnswer: {answer[0]}\\nResponse: {responses[0]}\\nExtracted: {extracted_responses[0]}\")\n",
    "    print(''.join('✅' if r == a else '❌' for r, a in zip(extracted_responses, answer)))\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klJabrnL7-tP"
   },
   "source": [
    "# Correctness function that shows multiple generation\n",
    "# Cut and paste this into the code section to use it\n",
    "\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the answer is correct.\"\"\"\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    \n",
    "    # Print all responses\n",
    "    for i, (response, extracted) in enumerate(zip(responses, extracted_responses)):\n",
    "        print(f\"\\nGeneration {i+1}:\")\n",
    "        print(f\"Question: {prompts[0][-1]['content']}\")\n",
    "        print(f\"Answer: {answer[0]}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Extracted: {extracted}\")\n",
    "    \n",
    "    # Compare each response with the answer\n",
    "    print(''.join('✅' if r == answer[0] else '❌' for r in extracted_responses))\n",
    "    return [2.0 if r == answer[0] else 0.0 for r in extracted_responses]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxRBgFxbuLd2"
   },
   "source": [
    "# Traning Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_6CGOsavKeTO"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# model_name = \"Qwen/Qwen2.5-0.5B\"\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "output_dir = f\"outputs/{model_name.split('/')[-1]}-GRPO\"\n",
    "run_name = f\"{model_name.split('/')[-1]}-{dataset_name.split('/')[-1]}\"\n",
    "\n",
    "\n",
    "# Set memory-related environment variables\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnACYvTBWA1q",
    "outputId": "8c9bf798-cf8c-4dea-cef1-fe85eaaa8d62"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length=256\n",
    "max_completion_length=512\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                             # Rank of the LoRA decomposition (low-rank matrix size)\n",
    "    lora_alpha=64,                   # Scaling factor for the LoRA update\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],            # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",           # Tells PEFT it's for autoregressive language modeling\n",
    "    lora_dropout=0.05,               # Dropout applied to LoRA layers during training\n",
    ")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=1e-5,\n",
    "    beta=0.005, # divergence coefficient – how much the policy is allowed to deviate from the reference model. higher value – more conservative updates. Default is 0.04\n",
    "    optim=\"adamw_8bit\",\n",
    "    adam_beta1=0.9,\n",
    "    adam_beta2=0.99,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4, #batch\n",
    "    num_generations=4,  # group size\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_completion_length,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=10,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    log_on_each_node=False,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\", # T4 is not supported\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=training_args.max_completion_length,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        correctness_reward_func,\n",
    "        format_reward_func\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEGtmm2rTW8Q",
    "outputId": "85638895-c2fe-4dfc-f7de-33bdf639cb67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L4I4UawZuRF3",
    "outputId": "7600b089-afe4-4601-e266-36eb0bbec017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Ahmed and Emily are having a contest to see who can get the best grade in the class. There have been 9 assignments and Ahmed has a 91 in the class. Emily has a 92. The final assignment is worth the same amount as all the other assignments. Emily got a 90 on the final assignment. What is the minimum grade Ahmed needs to get to beat Emily if all grades are whole numbers?\n",
      "Answer: 100\n",
      "Response: <reasoning>To determine the minimum grade Ahmed needs on the final assignment to beat Emily, we need to consider that Ahmed's maximum possible grade on the final assignment is 91 (his last possible score), and Emily's maximum possible grade is 100 (her last possible score).\n",
      "The difference between Ahmed's and Emily's maximum possible grades is 100-91 = 9. This would occur if Ahmed gets a perfect score on the final assignment. To beat Emily, Ahmed needs a grade that is at least as high as Emily, which is 90.</reasoning>\n",
      "<answer>89</answer>\n",
      "Extracted: 89\n",
      "❌❌❌❌❌❌❌✅✅❌✅❌❌❌❌❌\n",
      "❌⭐❌❌⭐❌⭐❌⭐❌⭐⭐❌❌⭐❌\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='1868' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  13/1868 10:32 < 29:38:52, 0.02 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>-0.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.137900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In a graveyard, there are 20 skeletons.  Half of these skeletons are adult women, and the remaining number are split evenly between adult men and children.  If an adult woman has 20 bones in their body, and a male has 5 more than this, and a child has half as many as an adult woman, how many bones are in the graveyard?\n",
      "Answer: 375\n",
      "Response: To solve this, let's first determine the number of each type of skeleton:\n",
      "- Total number of skeletons = 20\n",
      "- Number of adult women = 20/2 = 10\n",
      "- Remaining number of skeletons = 20 - 10 = 10\n",
      "- Number of adult men = 10/2 = 5\n",
      "- Number of children = 10/2 = 5\n",
      "\n",
      "Now, let's determine the number of bones for each category:\n",
      "- Each adult woman has 20 bones, so 10 adult women have 10 * 20 = 200 bones.\n",
      "- Each male has 5 more bones (than an adult woman), so 5 adult men have 5 * 20 = 100 bones.\n",
      "- Each child has half as many bones (than an adult woman), so 5 children have 20 / 2 = 10 bones.\n",
      "\n",
      "The total number of bones in the graveyard is equal to the sum of the bones of each category:\n",
      "200 + 100 + 10 = 310\n",
      "\n",
      "So, 310 bones are in the graveyard.\n",
      "Extracted: To solve this, let's first determine the number of each type of skeleton:\n",
      "- Total number of skeletons = 20\n",
      "- Number of adult women = 20/2 = 10\n",
      "- Remaining number of skeletons = 20 - 10 = 10\n",
      "- Number of adult men = 10/2 = 5\n",
      "- Number of children = 10/2 = 5\n",
      "\n",
      "Now, let's determine the number of bones for each category:\n",
      "- Each adult woman has 20 bones, so 10 adult women have 10 * 20 = 200 bones.\n",
      "- Each male has 5 more bones (than an adult woman), so 5 adult men have 5 * 20 = 100 bones.\n",
      "- Each child has half as many bones (than an adult woman), so 5 children have 20 / 2 = 10 bones.\n",
      "\n",
      "The total number of bones in the graveyard is equal to the sum of the bones of each category:\n",
      "200 + 100 + 10 = 310\n",
      "\n",
      "So, 310 bones are in the graveyard.\n",
      "❌❌❌❌❌✅❌❌❌❌❌❌❌❌❌❌\n",
      "❌⭐❌⭐⭐❌⭐⭐⭐⭐❌⭐❌❌⭐⭐\n",
      "Question: Marie has 98 unread messages on her phone. She decides to clear them by reading 20 messages a day. However, she also gets 6 new messages a day. How many days will it take her to read all her unread messages?\n",
      "Answer: 7\n",
      "Response: <reasoning>To determine how many days it will take Marie to read all her unread messages, we need to consider the total number of messages she will read and her reading speed. For each day, she reads 20 messages plus the 6 new messages, which totals 26 messages per day. Given that she has 98 unread messages, we can calculate the number of days it will take her to read all of them by dividing the total number of unread messages by the number of messages she reads each day: 98 / 26 = 3.82. Since she can only read full days, we round up to the nearest whole day.\n",
      "\n",
      "However, it's important to note that she also receives 6 new messages each day, so she might need to count her upcoming messages rather than just those that have already been cleared. Therefore, we should consider if her total unread messages increase over time. With 6 new messages added each day, we should multiply the initial number (98) by the increase rate (6) to find the total increase in unread messages each day: 98 * 6 = 6\\*98 = 984 messages per day.\n",
      "\n",
      "Now we add the two processes (read messages and increase unread messages by new messages) and divide the new messages by the daily read rate: 984 / 8 = 123.5 days.\n",
      "\n",
      "Since she can only read complete days, we round up to the nearest whole day: 124 days.\n",
      "\n",
      "Therefore, it will take Marie 124 days to read all of her unread messages.</reasoning>\n",
      "<answer>124</answer>\n",
      "Extracted: 124\n",
      "❌❌❌❌✅✅❌❌✅❌✅✅✅❌❌✅\n",
      "❌⭐❌❌⭐⭐⭐❌⭐⭐⭐⭐❌❌❌⭐\n",
      "Question: Nina makes one-of-a-kind jewelry and sells it at a local boutique.  She charges $25.00 for her necklaces, $15.00 for bracelets and $10.00 for a pair of earrings.  Over the weekend, she sold 5 necklaces, 10 bracelets, 20 earrings, and received 2 orders for a complete jewelry ensemble that Nina charged $45.00 for.  How much money did she make over the weekend?\n",
      "Answer: 565\n",
      "Response: To calculate Nina's total sales for the weekend, we first need to calculate the revenue from each type of jewelry sold. \n",
      "For necklaces: 5 necklaces x $25.00 = $125.00\n",
      "For bracelets: 10 bracelets x $15.00 = $150.00\n",
      "For earrings: 20 earrings x $10.00 = $200.00\n",
      "For complete jewelry ensembles: 2 orders x $45.00 = $90.00\n",
      "Now let's add up all the revenue from sales: $125.00 + $150.00 + $200.00 + $90.00 = $565.00\n",
      "To find the total amount of money Nina made over the weekend, we need to take her total sales amount and subtract the sales amount for orders she didn't sell. This would be $565.00 - $90.00 = $475.00\n",
      "The answer is $475.00.\n",
      "Extracted: To calculate Nina's total sales for the weekend, we first need to calculate the revenue from each type of jewelry sold. \n",
      "For necklaces: 5 necklaces x $25.00 = $125.00\n",
      "For bracelets: 10 bracelets x $15.00 = $150.00\n",
      "For earrings: 20 earrings x $10.00 = $200.00\n",
      "For complete jewelry ensembles: 2 orders x $45.00 = $90.00\n",
      "Now let's add up all the revenue from sales: $125.00 + $150.00 + $200.00 + $90.00 = $565.00\n",
      "To find the total amount of money Nina made over the weekend, we need to take her total sales amount and subtract the sales amount for orders she didn't sell. This would be $565.00 - $90.00 = $475.00\n",
      "The answer is $475.00.\n",
      "❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌\n",
      "❌⭐❌❌⭐❌❌⭐❌⭐⭐⭐⭐❌⭐⭐\n",
      "Question: Mary bought a packet of 1500 stickers. She shared them between Susan, Andrew and Sam in the ratio 1:1:3 respectively. If Sam gave Andrew two-thirds of his own share, how many stickers does Andrew now have?\n",
      "Answer: 900\n",
      "Response: The total ratio representing the stickers is 1+1+3 = 5\n",
      "In the ratio 1:1:3, Susan's share is 1/5, Andrew's share is 1/5 and Sam's share is 3/5.\n",
      "So, Sam's share is 3/5*1500 = 900 stickers.\n",
      "Sam gave Andrew 2/3 of his share, so Andrew now has 900*2/3 = 600 stickers.\n",
      "\n",
      "This task involves ratio calculation, sharing stickers, and multiplication.\n",
      "The answer is 600.\n",
      "Extracted: The total ratio representing the stickers is 1+1+3 = 5\n",
      "In the ratio 1:1:3, Susan's share is 1/5, Andrew's share is 1/5 and Sam's share is 3/5.\n",
      "So, Sam's share is 3/5*1500 = 900 stickers.\n",
      "Sam gave Andrew 2/3 of his share, so Andrew now has 900*2/3 = 600 stickers.\n",
      "\n",
      "This task involves ratio calculation, sharing stickers, and multiplication.\n",
      "The answer is 600.\n",
      "❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌\n",
      "❌⭐❌❌⭐⭐⭐❌⭐❌❌⭐⭐⭐❌❌\n",
      "Question: Sebastian plays drums for percussion in an orchestra. He sits next to the brass section where four people play trombone, two play trumpet, and one plays a French horn. In front of them are the strings section with three violinists, a cellist, and a contrabassist. Next to the strings are a trio of clarinets and a quartet of flute players in the woodwinds section. The maestro conducts them all. How many people are in the orchestra?\n",
      "Answer: 21\n",
      "Response: To determine the total number of people in the orchestra, we need to count the members of each section and then sum them up. Here's the breakdown of the various sections:\n",
      "\n",
      "1. Percussion section:\n",
      "   - Sebastian plays drums\n",
      "   - The brass section has four people (trombone, trumpet, French horn)\n",
      "   - The strings section has three violinists, a cellist, and a contrabassist\n",
      "\n",
      "2. Strings section:\n",
      "   - There are three violinists, one cellist, and one contrabassist\n",
      "\n",
      "3. Woodwinds section:\n",
      "   - There are a trio of clarinets and a quartet of flute players\n",
      "\n",
      "Combining the information from the percussion, strings, and woodwind sections, we can calculate the total number of people in the orchestra:\n",
      "\n",
      "\\[\n",
      "\\text{Percussion} + \\text{String section players} + \\text{Wind section members} = \\text{Total people in the orchestra}\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "2 + 3 + (3 + 1) = 2 + 3 + 4 = 9\n",
      "\\]\n",
      "\n",
      "So, the total number of people in the orchestra is **9**.\n",
      "Extracted: To determine the total number of people in the orchestra, we need to count the members of each section and then sum them up. Here's the breakdown of the various sections:\n",
      "\n",
      "1. Percussion section:\n",
      "   - Sebastian plays drums\n",
      "   - The brass section has four people (trombone, trumpet, French horn)\n",
      "   - The strings section has three violinists, a cellist, and a contrabassist\n",
      "\n",
      "2. Strings section:\n",
      "   - There are three violinists, one cellist, and one contrabassist\n",
      "\n",
      "3. Woodwinds section:\n",
      "   - There are a trio of clarinets and a quartet of flute players\n",
      "\n",
      "Combining the information from the percussion, strings, and woodwind sections, we can calculate the total number of people in the orchestra:\n",
      "\n",
      "\\[\n",
      "\\text{Percussion} + \\text{String section players} + \\text{Wind section members} = \\text{Total people in the orchestra}\n",
      "\\]\n",
      "\n",
      "\\[\n",
      "2 + 3 + (3 + 1) = 2 + 3 + 4 = 9\n",
      "\\]\n",
      "\n",
      "So, the total number of people in the orchestra is **9**.\n",
      "❌❌❌❌❌❌❌❌✅❌❌✅❌❌❌❌\n",
      "❌❌❌❌⭐⭐❌⭐⭐⭐⭐⭐⭐⭐❌❌\n",
      "Question: A thirsty traveler found an oasis in the desert. He drank 32 ounces of water. His camel drank seven times as much as he did. There are 128 ounces in a gallon. How many gallons of water did they drink altogether?\n",
      "Answer: 2\n",
      "Response: <reasoning>To find the total amount of water they drank, we first need to calculate the amount each person drank and then convert it into gallons. The traveler drank 32 ounces and the camel drank seven times as much as the traveler, which is 32 * 7 = 224 ounces. Together, they drank 32 + 224 = 256 ounces. There are 128 ounces in a gallon. To convert ounces to gallons, we divide by 128. So, 256 / 128 = 2 gallons.\n",
      "</reasoning>\n",
      "<answer>2</answer>\n",
      "Extracted: 2\n",
      "✅✅✅✅❌❌❌❌❌❌❌✅❌❌✅✅\n",
      "❌⭐⭐⭐❌⭐⭐⭐❌❌⭐❌❌⭐⭐⭐\n",
      "Question: Jonsey is awake for 2/3 of the day and spends 1/2 her time awake playing outside and the rest inside. Her brother, Riley, is awake for 3/4 of the day and spends 1/3 of his day outside and the rest inside. How much time, on average, do they spend inside?\n",
      "Answer: 10\n",
      "Response: <reasoning>In total, Jonsey and Riley spend 2/3 of the day awake. Jonsey spends 2/3 * 1/2 = 1/3 of this time playing outside. The rest, a quarter, is spent inside, meaning they spend 1/3 - 1/3 = 0, which results in 1/3 * 3 = 10 minutes. Similarly, Riley spends 1/3 of the day awake. Riley also spends 1/3 of his day inside, meaning Jonsey spends 2/3 - 1/3 = 1/3 of the day outside, resulting in 1/3 * 3 = 10 minutes. Thus, Jonsey and Riley spend a total of 10 + 10 = 20 minutes inside. </reasoning>\n",
      "<answer>20</answer>\n",
      "Extracted: 20\n",
      "❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌✅\n",
      "⭐❌❌❌⭐⭐❌⭐❌❌⭐❌⭐❌⭐⭐\n",
      "Question: Frank invites his friends over to play video games. He bakes a pan of brownies before he arrives. He cuts 6 even columns and 3 even rows into the pan of brownies. If there are 6 people, including Frank, in total, how many brownies can they each eat?\n",
      "Answer: 3\n",
      "Response: <reasoning>To determine how many brownies each person can eat, we first need to calculate the total number of brownies. We do this by multiplying the number of columns by the number of rows: 6 columns * 3 rows = 18 brownies.\n",
      "Since there are 6 people, including Frank, in total, we divide the total number of brownies by the number of people: 18 brownies / 6 people = 3 brownies per person.\n",
      "The reasoning process here is to multiply the number of columns by the number of rows to calculate the total number of brownies, and then divide this total by the number of people to find out how many brownies each person can eat. The answer is 3.\n",
      "Extracted: <reasoning>To determine how many brownies each person can eat, we first need to calculate the total number of brownies. We do this by multiplying the number of columns by the number of rows: 6 columns * 3 rows = 18 brownies.\n",
      "Since there are 6 people, including Frank, in total, we divide the total number of brownies by the number of people: 18 brownies / 6 people = 3 brownies per person.\n",
      "The reasoning process here is to multiply the number of columns by the number of rows to calculate the total number of brownies, and then divide this total by the number of people to find out how many brownies each person can eat. The answer is 3.\n",
      "❌❌✅❌❌❌❌❌❌❌❌✅❌❌❌❌\n",
      "❌❌⭐⭐❌❌❌❌❌❌⭐⭐❌❌❌⭐\n",
      "Question: James buys 3 CDs.  Two of them are 1.5 hours each.  The last one is twice that long.  What is the combined length of the CDs?\n",
      "Answer: 6\n",
      "Response: <reasoning>To find the combined length of the CDs, we need to add up their individual lengths: 2*1.5 + 1*1.5 + 2*2.5 = 3 + 1.5 + 5 = 9.5 hours.</reasoning>\n",
      "<answer>9.5</answer>\n",
      "Extracted: 9.5\n",
      "❌✅❌❌❌✅❌✅❌❌✅✅❌❌❌❌\n",
      "⭐⭐❌⭐❌⭐❌⭐❌⭐⭐❌⭐❌⭐❌\n",
      "Question: Cathy and Chris got summer jobs at the cake shop and were supposed to work 20 hours per week each for 2 months. During that time, Chris got sick for one week and Cathy took her shifts. If there are four weeks in a month, how many hours did Cathy work during the 2 months?\n",
      "Answer: 180\n",
      "Response: <reasoning>To calculate the total hours Cathy worked, we need to consider the total hours she worked for both weeks and weeks after.\n",
      "\n",
      "For the first 4 weeks (including the weeks Chris was sick):\n",
      "- Cathy worked 20 hours each week, so she worked 4 * 20 = 80 hours (including the sick day).\n",
      "- Chris worked 20 hours each week, so he also worked 4 * 20 = 80 hours.\n",
      "- Total hours for the first 4 weeks = 80 + 80 = 160 hours.\n",
      "- For the remaining 3 weeks (before and after Chris took his shifts), Cathy worked 20 hours each week, so she worked 3 * 20 = 60 hours.\n",
      "\n",
      "So, for the extra 3 weeks, Cathy worked 60 hours each day, totaling 3 * 60 = 180 hours.\n",
      "\n",
      "In total, Cathy worked 80 hours + 180 hours = 260 hours during the 2 months.\n",
      "\n",
      "</reasoning>\n",
      "<answer>260</answer>\n",
      "Extracted: 260\n",
      "❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌\n",
      "❌⭐❌⭐❌❌❌⭐❌❌❌❌⭐⭐❌❌\n",
      "Question: In the engineering department, 70% of the students are men and 180 are women. How many men are there?\n",
      "Answer: 420\n",
      "Response: <reasoning>To determine the number of men in the engineering department, we need to know the total number of students in the department and then calculate 70% of that total. We know that 180 women have been identified.</reasoning>\n",
      "<answer>180/0.7 = 257.14</answer>\n",
      "Extracted: 180/0.7 = 257.14\n",
      "❌❌❌❌✅❌❌✅❌❌❌❌❌❌❌❌\n",
      "⭐⭐❌⭐⭐❌⭐⭐⭐❌❌⭐⭐❌❌❌\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3730\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3732\u001b[0m             \u001b[0mloss_mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmp_forward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/extras/profiling.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mprofiling_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_prepare_inputs\u001b[0;34m(self, accumulated_local_batch)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgenerate_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffered_inputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;31m# self._buffered_inputs=None can occur when resuming from a checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                 \u001b[0maccumulated_local_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_and_score_completions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulated_local_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 self._buffered_inputs = split_tensor_dict(\n\u001b[1;32m    901\u001b[0m                     \u001b[0maccumulated_local_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/grpo_trainer.py\u001b[0m in \u001b[0;36m_generate_and_score_completions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    972\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_wrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgather_deepspeed3_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds3_gather_for_generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             ) as unwrapped_model:\n\u001b[0;32m--> 974\u001b[0;31m                 prompt_completion_ids = unwrapped_model.generate(\n\u001b[0m\u001b[1;32m    975\u001b[0m                     \u001b[0mprompt_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1876\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3434\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3436\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m    538\u001b[0m                     \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflash_attn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# Runs pre-forward logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;31m# Runs post-forward logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlora_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"dropout probability has to be between 0 and 1, but got {p}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m     return (\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m     )\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SijTzsmMjZBd",
    "outputId": "43482c60-eb22-4f3e-8d43-6f91511cc331"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('outputs/Qwen2.5-0.5B-Instruct-GRPO/merged/tokenizer_config.json',\n",
       " 'outputs/Qwen2.5-0.5B-Instruct-GRPO/merged/special_tokens_map.json',\n",
       " 'outputs/Qwen2.5-0.5B-Instruct-GRPO/merged/vocab.json',\n",
       " 'outputs/Qwen2.5-0.5B-Instruct-GRPO/merged/merges.txt',\n",
       " 'outputs/Qwen2.5-0.5B-Instruct-GRPO/merged/added_tokens.json',\n",
       " 'outputs/Qwen2.5-0.5B-Instruct-GRPO/merged/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Your last checkpoint\n",
    "checkpoint_path = f\"{output_dir}/checkpoint-10\"\n",
    "\n",
    "# Load adapter config\n",
    "peft_cfg = PeftConfig.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_cfg.base_model_name_or_path)\n",
    "\n",
    "# Load adapter onto base\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
    "\n",
    "# Merge adapter into base weights\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model (now a standalone full model)\n",
    "merged_path = f\"{output_dir}/merged\"\n",
    "model.save_pretrained(merged_path)\n",
    "\n",
    "# Save tokenizer (so vLLM can tokenize properly)\n",
    "tokenizer.save_pretrained(merged_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFRHLvWhzHn0"
   },
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e316dcdf8e7f4d7689f9875879ce8a45",
      "b4317a4568c54bf09c8925fec0f12fa7",
      "5154ed7f87bb4604a83fe8d818f0edbc",
      "6c651ed6cc1f492694d67fffb2c23fea",
      "096566118949404780ee8a5ecc49d3bc",
      "f573a99e5c1f4a638f9d97b7014e9bc2",
      "f308d4d1801f4454831db99011f92f0d",
      "3edf63145271492ca09af32abcd25615",
      "ebc8792268a4482d995c5a5c72915d0d",
      "3628f2b846394540b238dcf404409c44",
      "1cd75f9a0da1498d932c145fca78ca49",
      "76c761bcb06044a6b2dcd65d5721768a",
      "c04e427311fa435494cc247c4f27861b",
      "7917b87a0f454e588abf5b301e18abb4",
      "e982191f74454c5bbb22bbd5f2d58642",
      "09c2e62791e44a9a920a0f1734f98389",
      "408b7f55651a420b85d498d02d8bae88",
      "72f7e7ca924f4fb79a12d77138a86092",
      "e3d5e6fc522a4732bfe83351b555f41d",
      "8d7a9a7015814d7682d3adfb5fdde485",
      "e09b1b1d04a047ce969f5dec0495b2f7",
      "0983944fb4c04fdfaad8247df5a9c713",
      "1b4aa0f27a7c417f9d5769d68c90c9ce",
      "732c740b65d2449987bb79f0d8236c0e",
      "e21dddf0cb734a77a24073566f4d33ef",
      "921c16a57126440bb2c303e77986785b",
      "3c549a29dd974499b40ba047a6b4c550",
      "0e0d27d5edd74bf8bff7673d4d5767c3",
      "f69a70f78d2d43709028e8cdd6aa3927",
      "0ac3925d65274f38a00530d586ca9cab",
      "9e07e4c0fdcf4a29b705d72f04442f93",
      "de8bfafc3c454ac1ab0f2db29d45576f",
      "087cdc7893744fd39cb666e559ab0673",
      "23a56fcfcb4149a286fdccb787dbd2ee",
      "675e6e8395d942229402721592ffc39e",
      "247d6a7d832e48bdb4ec78d1f9bc9d10",
      "5b564c477234494085e6d7836dc98af3",
      "5b1df8c236ec4dd0b95de27c0621ab20",
      "e122390779974f1291f279e384b93038",
      "264a1700f54540939501fe5bbf58d9b1",
      "a56a4c509c1344c79dd9592c4b1c7f86",
      "661c1933772742ca9a77f9779be728c6",
      "e18804481c1a40e590c16042e28d1206",
      "70f62aba23a2471387ac4d069c5df9a3",
      "60fb047252654042a0c10ae0af7258bc",
      "a6b1a2eaa72946dc9b00c8f3649d48a7",
      "a87846bd2c1a4b8ba9c62bbc5503c655",
      "aada8353f20d4c13a083cf1ab2e8374d",
      "2fabe48e79614d50af7ffbf7229c222a",
      "4242eec6cbb245609ce3f9448271b442",
      "b4b4e560e5294c209100fef7f368ef61",
      "c07168097d564384b96872b4bd8f9e89",
      "b87954f5e0b94bdf94be3525efa145d1",
      "53b6e8e9d8b246c28b44717bf62c59bf",
      "1a390c6f4aaf482ea4b49267d65cd6fe",
      "0464f89ab7b642e2ae27f0100f87de47",
      "5405936b45f644518632499f4ed86dec",
      "4539c2cb20a641878723e5c0ea327b87",
      "32079fa7cb5940e49773cbf8130cdb47",
      "9b0057f2b1f3453a85ff31f1b79db757",
      "f1b2e743bbeb42538b91ad441f23d709",
      "99734c5827c3456b8c25cd30c03296c8",
      "16b3b8b6c25946c6828afcac800220cf",
      "c301dc48fb034f499e3b56f3bf2332bf",
      "06ab9062dbbf4ab1bd00b1c57c836c71",
      "f0f3b321a5d6412496611a8b542df34d",
      "0c574e60f2dd4089ae612eb9f327896d",
      "bdfdb7f492934a7f94075293883fe257",
      "69308bbd8a2a400a9900d06d2251a976",
      "078888f3212b42dcaf33410b946bba82",
      "be634cacead74e0d90bc6b61ba88ac63",
      "06a3eb85e8c74d0bbfdf856a15521cd1",
      "da68fc86b031444bae0c94d9169b71c3",
      "c0e7e6b7b897405197c1f34ea922f697",
      "6097c148afb54f879dd255256bd9e5ca",
      "453ac30551084ae98b4b91229259ff0f",
      "ce0744f9e4664f42a14edf793392cecb",
      "1b1e410355a745e99205c3dea07104f2",
      "98d5859f8afd412681e25f14b50363f9",
      "ed55b49951344c58b3d3e44377956d2b",
      "04998f4640b844d596a4c4640a009cbe",
      "01a21c62315445a69489cd27f3e8f789",
      "6c819dad9ce445d5b0f0dc386efc796a",
      "3ceb39eb31d9441a8d2a66c770da8998",
      "92c7544f751343809fdeb17eeb0073ff",
      "b2b8d005b3344e529ac8903a15ab595a",
      "d769a466afb44503804ef9b266eeec24",
      "aa969a0077624953862e81baa1f55c0d",
      "a9e45dcbdafe4945b40093307d033546",
      "540e36e7588c41cd81cf48bf8f296f65",
      "85d362babd424249bc882615c68ce2ef",
      "8402f00537dd41ffa0c817879675a856",
      "73c326578fd94357a2a3d0c28858933e",
      "7d67dd87178c424f833f55301c8b4e4c",
      "4117dc61e13548e3ab3aca467bf5f009",
      "926f6a70e1d148a5a5f35c98f71fa9d8",
      "f2c49c4bdb054bb1a3c07e9d1be8375b",
      "e3e116d4b6994590935349b5ca230555",
      "0ed796ac8878401fb4e56351675cd66a",
      "efd54ec3cd1b429683cbaf3fbc0af91f",
      "0cd346803db64a7fb62ea5122f88f19d",
      "db03e461deef40a7be115743116d3d28",
      "f8f55ca33f4e400ebaf096ced270d059",
      "32908d2c2c4145b48a2052cbac2164e4",
      "8c5101bcfb3d4392a5452580af76d7c4",
      "4545b69e245e4594822ff2f89862426c",
      "bd850d728a114f5aa80b463cb5ff4d5f",
      "b7f2672a1ccf4480b1dc0ade90c04533",
      "cc6e7bb83b1a454e816da0757a6ad99a",
      "b48985ee41fa405294c425547e295aa9",
      "d437336ed42746c086e3ee2bdb888c1d",
      "49ae2e597e764913882bca8dc48c30a7",
      "d3bba8266ec1401fbfa5d4c25d985a92",
      "aa11dcb499514c6a885123e32af3dd74",
      "d8653cd684fa47c6b0d3c39ba40a673f",
      "a1a1f195c671441eb579bbfb469be6b7",
      "fc4d858eb3544ce1bb24b070fdf71b29",
      "8204da1d8ab44815b49194bd912477a2",
      "6584c419d4b14c238076c5c2abcb00e5",
      "c551d02769e7496697a73032f4c1ed65",
      "8cb9738425784c40b3032613ce814b38",
      "2902bd45f2424c6cbc39bf8de09e1e3e",
      "6a2a55afb43e4308be66bd971d2bafee",
      "298f045602184d779fdf496d4d49496b",
      "67529275a5b5481184c3e9ce70e3c9cf",
      "8ee0c4da576345f1a0555d8abb3da803",
      "5a1a8f5c04b34503bd42f19c5ea1eb8e",
      "a2db86b0b1e54ae8abe342666a8e6ab8",
      "6d752c59aa9f4afca42a8411d2d03085",
      "237d9a8ed6ea41be82e957a28570bac0",
      "18dcdad6a0234ccc83aa999c707db20f",
      "225d1ecfd2044f9aae1325070f316457",
      "7f4f71840eb14dfd9de1a03a4f254c3f",
      "0290ef08733e44fdbb3d4502521ff63e",
      "c9162564d34646bfa0ef345cf8db4b01",
      "06b90dafa9c54859814624aba3e728d1",
      "52045e6316d3446981f16d1d739f25a3",
      "0235445e657140179ff2618e7764ce53",
      "a72bc31480c04218acd7bb91f766d051",
      "bf6eeba899e64c259d910b3ce473e0d8",
      "bf89a0636aad4abda2832d6703e76a06",
      "d90902287ac44690aa8419eb1baf3064",
      "c97f5ba9780f447fb8dc4b1c2cfb4a2a",
      "6bbb7e390abf4f2c910f6231b2c4b2b1",
      "a4caf80f2d26428f8856b46b3caa2095",
      "b18c8b07355c4700bae84876e88cd459",
      "deb76872e6b1432bbfb73a4349c0d305",
      "c3b2e9c7a3bb4e5081a39a2ae6481cf4",
      "65e5258f5fb842e9bb1ebc34fe7ce8af",
      "c7ee843f3ef64f00971f8257090a77af",
      "82bff0ba26584193b94b602cc3db773c",
      "92a2171da5d649bb9f5e8093f3494224",
      "2dd4a640f95a4e62ac9376db14328430",
      "ef5b0dced3fa4055a37a18a0fe498d2a",
      "10f0310412c148358dd1f41dab1a7887",
      "caf6e9d73b654a31b68de425078d0b80",
      "159de61d6ad1499da90f2707424fec76",
      "bde7320ed6fe40b49765acd54e8ca4b6",
      "1c6e996407344d7f8bffbfe4b6208069",
      "fc065f3d9bb34b29a716683a2aea8ad4",
      "278a464ecd3c476b9b12d1f3f1599eee",
      "228812390173438680e894492f4b1153",
      "864579c20d3943ebad22647cba206449",
      "2b34a268082145519039575fe6156809",
      "3df7b8cb2001481287237bfb091e5ad8",
      "849a12cf17d4461183ba0c33a9530db1",
      "d82f4f2058ec4f16ae598cf21235b226",
      "7769f2b85e9e482a895d619d4023166a",
      "b6127feb0cb744d59ed82fcad03206c5",
      "2fbb3aff712a460b86fc39c3f37b4aa6",
      "5319fda84ce44b5486ef8b19fe14370f",
      "de86f23625f44bed89c2a84c2e8cb788",
      "9431e9d0c39d44cc96fd0fa656d9fd27",
      "65bd7b2d9b6940bc8b79a43cd430aa34",
      "4b5976b5a774467e9e32d027d6bd7fff",
      "a979f7b97bef4dca823e4e9249557de9",
      "4fa4d304833d4a60887bd93529a16126",
      "032b91b7a5b24e768571f1cb036d086f",
      "479b03adc5f9408b98c5e22b381f31ef",
      "859815ea534949319dd6d232434b56cf",
      "cf5b839c194946448e00d689a2d782a0",
      "899a16c296fb4cfaa3e913350698c4ed",
      "550c2e9a3932470886c5cd4d18a457c3",
      "fb51d91c899647c6a47edec2067ca295",
      "540b6c9d62ab4efcaa15a995f3230016",
      "e9939250770a423c9c067b17ad5c67dc",
      "175ca9a692094f4bb7c076a69e148aa2",
      "3a06f9139ea04d3abe208f4d2478d4e0",
      "5e803c2c1e6143f0980fa6905fca79e9",
      "11ac3e8693b24a05b5c7ece26880885c",
      "364e3be584a14e91a8686959c92a1f7f",
      "a7b1fd2ae52c4f40a82ab28a486a86b6",
      "cdf43ad76a1348dab4afe93c2f4c55f4",
      "6cfeae7a800d4be98ba571dbc9025998",
      "6390de4b42e04dadb5798c7f47a9cce9",
      "85efa962b2464f47b38c65520288b284",
      "c1c9f648ce8b4bf59c38406cfdd7d3c6",
      "bbf0b7ca3cbf422e8795b388e65a903d",
      "db073ed232d94ec2a16157d4558dde3a",
      "4ba11625400e4623afa90a4aa7a4db09",
      "2c4def23b91a4d0590b38976f926afcc",
      "b26a3991727944518121aacfe75c8f67",
      "919776ac61b34cd0a61dafa55bbf47ae",
      "ef4910b9e65b4f1abfa933aee2dfe8f3",
      "5242af22649e48a7a1310458f2122173",
      "571a3cfd9ee444088aa12d47da5e96b8",
      "bb64afff020043b4805542b9bd7191f9",
      "9d90575b8b4248c0bee379a8158cccb4",
      "7d192a3d4bac4354b77cd046c1130569",
      "e802e51d41154b87b076462a585d9518",
      "ab5c02c4d1994f90ba6513dd55dc4884",
      "aaac3c781f2347dabe273e6fa2a1ff53",
      "15d3ecf8b6874c9bafb3b39b436ff236",
      "00e743330cfb4435a0899a5c18ee8313",
      "b4623bcbba124cf28fc15920523918bd",
      "db6dcc104a09437db0054b2b75dbcc21",
      "8c7a3a61f3814f87ae3875522b4e2375",
      "08eef0818dc744f8bc1935abad699278",
      "307a6a70fe0f474a9649bc4a2a4ec4d8",
      "912278c9b6394e589854e026c0275d55",
      "5754d2f1e02c4b99802b5312765cd343",
      "1bd95a6b302c49a080ff279cc915b726",
      "bf73d60f2ec542a092fe43ab1ec4ef47",
      "cbd66b3fa6af42a182901fbaf8e7bf32",
      "9dc9e5b9f7544bc5ba2d13ead32a034a",
      "00ef6f6b22dc41e98bd49a0f5f412761",
      "a8dea6576e334f45a6e8b6274e79420d",
      "9e92406f8442404790ceddbdc73ef201",
      "13a27a3255594904b09257cac3991097",
      "f46912823f414df587c8dbd2ef103c46",
      "37a50fa766ad487b8ec250a4109a0cbc",
      "2a6442e62a0f4650afb01c11e73d423a",
      "bffc438caa1048f583c264fa81968cda",
      "088eccc6fc95497094b1177a30808d98",
      "5e939e771953411bbfc8b46f7da1c7be",
      "4cef365227c840f6ab86383963b6af73",
      "86afae28e1d74cbaaa79ba045d3fea71",
      "05584c97c6314b6bb14a9dfb4dbb55fe",
      "5970d1803e224b3eb3f9042568499b0c",
      "263bb82cdf704ac6a8e3d5d641bc7624",
      "785429e9d37245ad83ddbd74425cb62e",
      "4803800749ee414c932b5229c0bf421d",
      "071bffb94c8647c6b57c2a9fcf9ce162",
      "ab5a61e37d0b447b9745d1d7b441ab0a",
      "d2fa6297214242b28c6f72139cdd0eda",
      "5b931f90f8854794bffec3ba77203736",
      "c368710dc99540caa7c40f8cbe904d9d",
      "36c8e21e08eb4890b7469e18187b0c75",
      "d9bedb409d1040a695d63f417c884dc8",
      "72a004eb37414a33b0d0f33f51fa3cd3",
      "4f4178ceff4944b7a1b5bd4e020f31ca",
      "a453576c505441cb9251cdfaeda77cea",
      "2126092b39b84c398f2852fc83395955",
      "cc26e8bc160e4a0dafbac90510d80546",
      "2a87ed09c0a9400187da2d9fcb5fd126",
      "752adb48644846d6a6c2b7b34ce2059f",
      "6124c15cc3ba45379c36d916da51e72d",
      "e253756cb3e84fcebcc6e143d111b18e",
      "986e1bdc05544ded90d0b65bd007558a",
      "726fe323f6904eaeb76e19ce7e55ef07",
      "fac91f73f454439f920c6463ef893408",
      "5be299b67a504e00931b5ffd0344871b",
      "2eaa0f75b48f495c8ee7c35664d97771",
      "5793455c0459480d9182574ad9f2334c",
      "07bdf7b525ec4777964943db01dacd73",
      "a447519aeb89437e80481073df504a21",
      "64a6167691a14e0e971630d5dda7c75f",
      "ca4528e41df941e5904dd414ff848624",
      "9af1f4b481e04487b50c51101e3697f6",
      "b48eda38bd9748848fe4bd4045a627e2",
      "a378e5ee5f654cc5823c8b29edf5f51a",
      "8a486a6056244735b00938e324674c42",
      "819b5357cc854e71a4cc8c02b4be3231",
      "01f12bcd3d2f412da56e87ff49d9cc6a",
      "c8383a88088d4c879ade9eaa46dc9383",
      "da964318d9284881af7d6a4ebdc9ea80",
      "6d45d25514264318bef4783560fad106",
      "3342e33f36bc4c20846c20da3abc73c5",
      "38e4bd85fde245e19e4cbb1a7ac8837a",
      "3004bcffe420435fa91c39945b884889",
      "7e50f267adfd4b1cbeb867e1b8fe09c1",
      "febbea75dc1145a19334fc28468d172f",
      "99bb0aea0ccc417a87d3a4589de465ec",
      "0ec6402ab25349c08a2a9318b7a50f06",
      "7a2f56f75b3145b1993a484f03025fe6",
      "4eb9baff21ea448c83089aaf9c683494",
      "fcbce5160beb4e93b75cf826e394a410",
      "a9a2a4352ce54fea84a01640de0542f0",
      "1c52356531e84d41a8e876f7f8ccdf1f",
      "13b0b11f8c3a42379be9da3abbc65622",
      "5f235dcfb6fa46989143c94ca63be714",
      "11707ea4e1f54624a202758d66ce4077",
      "3312012eba19414fbe1b4a7713bd6ffb",
      "eb7247a0a65e4ea9b24cb0dbc6e590bf",
      "3939c694cc3b4b7aa9205b3a4f7f1434",
      "faf5ff27f0c245e6bf31be9eb4283401",
      "798001efe59c45d59b4d9ef465d035b0",
      "04dc8974a04d410ca7314244f9f82e3a",
      "66d951a22a354b0394ac2d3a5ca31e20",
      "12250961b65f4496b802e8cd83ff3fb5",
      "ac2a150b4735443d95f474134bcd4b5d",
      "02c50cbf4f264c088248762abdb881d9",
      "953134de4e5a49309ab739ee9914b9c7",
      "5dae40065ba6437095aab6d43a9da1c2",
      "b2938c0d9c8d4bd090f1dce25b73ddb8",
      "654d89431cb646d7a766dd4a1c187fa9",
      "09e25dcd4f1d4286b180a9467b82a85e",
      "ed90228696034260be2429da3efbf234",
      "fa5df364be264dc5a0c0453da2bc5771",
      "c87bb384855b42ab82acccd7e469dd17",
      "e45c079f1a3a480699616bacab7b1cc1",
      "f923265772684888ac5896043cc5342e",
      "a33534ada0ca4702928a5b9f42aaa87a",
      "eec170ef1d96474699395b5a0b38bcd2",
      "8870f93f4d974be4a57e99d826f5188a",
      "8511bb245c444bc0a568c1cfc419c458",
      "f4912705deba4f0ea90ea3fef854e2b7",
      "31d02d6d930c4f858f88c304a5c59fc8",
      "e8f9de2178634e679aa006e0dbcf19a2"
     ]
    },
    "id": "nW6pJMSDD2sv",
    "outputId": "d7d679a9-8e3e-4e81-ae50-58a60837ba8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 08:12:26 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-26 08:12:26 [__init__.py:239] Automatically detected platform cuda.\n",
      "Starting GSM8K evaluation...\n",
      "Initializing evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e316dcdf8e7f4d7689f9875879ce8a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading model components:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-26 08:12:52 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c761bcb06044a6b2dcd65d5721768a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4aa0f27a7c417f9d5769d68c90c9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 samples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a56fcfcb4149a286fdccb787dbd2ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing samples:   0%|          | 0/100 [00:00<?, ?examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fb047252654042a0c10ae0af7258bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0464f89ab7b642e2ae27f0100f87de47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c574e60f2dd4089ae612eb9f327896d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1e410355a745e99205c3dea07104f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e45dcbdafe4945b40093307d033546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd54ec3cd1b429683cbaf3fbc0af91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d437336ed42746c086e3ee2bdb888c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2902bd45f2424c6cbc39bf8de09e1e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4f71840eb14dfd9de1a03a4f254c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbb7e390abf4f2c910f6231b2c4b2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f0310412c148358dd1f41dab1a7887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849a12cf17d4461183ba0c33a9530db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa4d304833d4a60887bd93529a16126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a06f9139ea04d3abe208f4d2478d4e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db073ed232d94ec2a16157d4558dde3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e802e51d41154b87b076462a585d9518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5754d2f1e02c4b99802b5312765cd343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6442e62a0f4650afb01c11e73d423a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071bffb94c8647c6b57c2a9fcf9ce162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc26e8bc160e4a0dafbac90510d80546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bdf7b525ec4777964943db01dacd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da964318d9284881af7d6a4ebdc9ea80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcbce5160beb4e93b75cf826e394a410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dc8974a04d410ca7314244f9f82e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5df364be264dc5a0c0453da2bc5771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to gsm8k_eval_results_20250526_081430.json\n",
      "\n",
      "Final Evaluation Results:\n",
      "Accuracy: 19.00%\n",
      "Correct: 19/100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Disable VLLM's progress bars\n",
    "logging.getLogger(\"vllm\").setLevel(logging.WARNING)\n",
    "\n",
    "# Constants from training script\n",
    "R1_STYLE_SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it.\n",
    "The assistant first thinks about the reasoning process in the mind and then provides the user\n",
    "with the answer. The reasoning process and answer are enclosed within <reasoning> </reasoning> and\n",
    "<answer> </answer> tags, respectively, i.e., <reasoning> reasoning process here </reasoning>\n",
    "<answer> answer here </answer>.\"\"\"\n",
    "\n",
    "TASK_SPECIFIC_INSTRUCTIONS = \"The answer must be a single integer.\"\n",
    "\n",
    "def extract_xml_answer(text: str) -> str:\n",
    "    try:\n",
    "        answer = text.split(\"<answer>\")[-1].split(\"</answer>\")[0].strip()\n",
    "        return answer\n",
    "    except IndexError:\n",
    "        return \"\"\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    try:\n",
    "        return text.split(\"####\")[1].strip()\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "def evaluate_model(\n",
    "    model_path: str,\n",
    "    batch_size: int = 4,\n",
    "    num_samples: int = None,\n",
    "    save_results: bool = True,\n",
    "    gpu_memory_utilization: float = 0.3,\n",
    ") -> Dict:\n",
    "    print(\"Initializing evaluation...\")\n",
    "\n",
    "    # Initialize VLLM with progress indicator\n",
    "    with tqdm(total=2, desc=\"Loading model components\") as pbar:\n",
    "        llm = LLM(\n",
    "            model=model_path,\n",
    "            dtype=\"half\",\n",
    "            gpu_memory_utilization=gpu_memory_utilization,\n",
    "            max_model_len=768,\n",
    "            device=\"cuda:0\",\n",
    "            enable_chunked_prefill=True,\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_path,\n",
    "            model_max_length=768,\n",
    "            padding_side='right',\n",
    "            truncation_side='right'\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Set up sampling parameters\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=512,  # Matching max_completion_length from training\n",
    "        stop_token_ids=[tokenizer.eos_token_id],\n",
    "    )\n",
    "\n",
    "    # Load test dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    dataset = load_dataset('openai/gsm8k', 'main', split='test')\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(num_samples))\n",
    "    total_samples = len(dataset)\n",
    "    print(f\"Loaded {total_samples} samples\")\n",
    "\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Create progress bar\n",
    "    progress_bar = tqdm(\n",
    "        total=total_samples,\n",
    "        desc=\"Processing samples\",\n",
    "        unit=\"examples\",\n",
    "        dynamic_ncols=True,\n",
    "    )\n",
    "\n",
    "    progress_bar.set_postfix({\n",
    "        'acc': '0.00%',\n",
    "        'correct': '0',\n",
    "    })\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, total_samples, batch_size):\n",
    "        batch_data = dataset[i:i + batch_size]\n",
    "        current_batch_size = len(batch_data['question'])\n",
    "\n",
    "        # Prepare prompts using same format as training\n",
    "        prompts = [\n",
    "            [\n",
    "                {'role': 'system', 'content': R1_STYLE_SYSTEM_PROMPT + \"\\n\" + TASK_SPECIFIC_INSTRUCTIONS},\n",
    "                {'role': 'user', 'content': \"What is 2+2?\"},\n",
    "                {'role': 'assistant', 'content': \"<reasoning>To calculate 2+2, we simply add the numbers together: 2 + 2 = 4.</reasoning>\\n<answer>4</answer>\"},\n",
    "                {'role': 'user', 'content': q.strip()}\n",
    "            ] for q in batch_data['question']\n",
    "        ]\n",
    "\n",
    "        # Convert to chat format\n",
    "        formatted_prompts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                p,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            for p in prompts\n",
    "        ]\n",
    "\n",
    "        # Generate responses\n",
    "        outputs = llm.generate(\n",
    "            formatted_prompts,\n",
    "            sampling_params,\n",
    "        )\n",
    "\n",
    "        # Process responses\n",
    "        for j, output in enumerate(outputs):\n",
    "            response = output.outputs[0].text\n",
    "\n",
    "            # Extract answers\n",
    "            generated_answer = extract_xml_answer(response)\n",
    "            true_answer = extract_hash_answer(batch_data['answer'][j])\n",
    "\n",
    "            # Store result\n",
    "            result = {\n",
    "                'question': batch_data['question'][j],\n",
    "                'true_answer': true_answer,\n",
    "                'generated_answer': generated_answer,\n",
    "                'full_response': response,\n",
    "                'correct': generated_answer == true_answer\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            # Update metrics\n",
    "            if generated_answer == true_answer:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "        # Update progress\n",
    "        progress_bar.update(current_batch_size)\n",
    "        progress_bar.set_postfix({\n",
    "            'acc': f'{(correct/total)*100:.2f}%',\n",
    "            'correct': f'{correct}/{total}',\n",
    "        })\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': total,\n",
    "        'model_path': model_path,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Save results\n",
    "    if save_results:\n",
    "        save_path = f\"gsm8k_eval_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(save_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'metrics': metrics,\n",
    "                'results': results\n",
    "            }, f, indent=2)\n",
    "        print(f\"\\nResults saved to {save_path}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(\"Starting GSM8K evaluation...\")\n",
    "\n",
    "checkpoint_path = \"outputs/Qwen2.5-0.5B-Instruct-GRPO/merged\"\n",
    "\n",
    "metrics = evaluate_model(\n",
    "    model_path=checkpoint_path,\n",
    "    batch_size=4,\n",
    "    num_samples=100,\n",
    "    save_results=True,\n",
    "    gpu_memory_utilization=0.3,\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Evaluation Results:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2%}\")\n",
    "print(f\"Correct: {metrics['correct']}/{metrics['total']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmlnjGbJiI_w"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
